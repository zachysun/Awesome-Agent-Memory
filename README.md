<a name="readme-top"></a>

<h1 align="center">üß† Awesome Agent Memory</h1>

<p align="center">
    <b> A curated collection of systems, benchmarks, and papers et. on memory mechanisms for Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs), exploring how different approaches enable long-term context, retrieval, and efficient reasoning </b>
</p>

<details open>
  <summary>üóÇÔ∏è Table of Contents</summary>
  <ul>
    <li><a href="#systems">üß© Systems</a></li>
    <li><a href="#tutorials">üßë‚Äçüè´ Tutorials</a></li>
    <li><a href="#surveys">üìö Surveys</a></li>
    <li><a href="#benchmarks">üìè Benchmarks</a></li>
    <li><a href="#text-memory">üìù Papers - Text Memory</a></li>
    <li><a href="#parameter-memory">‚öôÔ∏è Papers - Parameter Memory</a></li>
    <li><a href="#multimodal-memory">üé• Papers - Multimodal Memory</a></li>
    <li><a href="#memory-retrieval">üîç Papers - Memory Retrieval</a></li>
    <li><a href="#articles">üì∞ Articles</a></li>
    <li><a href="#workshops">üí¨ Workshops</a></li>
  </ul>
</details>

<h2 id="systems">üß© Systems</h2>

#### Open-Source

_Ordered by the number of Github stars._

1. **[Mem0](https://mem0.ai/)**
   [[code](https://github.com/mem0ai/mem0)]
   [[paper](https://arxiv.org/abs/2504.19413)]
   [[blog](https://get.mem.ai/blog)]

2. **[Zep (powered by Graphiti)](https://www.getzep.com/)**
   [[code](https://github.com/getzep/graphiti)]
   [[paper](https://arxiv.org/abs/2501.13956)]
   [[blog](https://blog.getzep.com/)]

3. **[Letta (formerly MemGPT)](https://www.letta.com/)**
   [[code](https://github.com/letta-ai/letta)]
   [[paper](https://arxiv.org/abs/2310.08560)]
   [[research](https://www.letta.com/research)]
   [[blog](https://www.letta.com/blog)]

4. **[Second Me](https://home.second.me/)**
   [[code](https://github.com/mindverse/Second-Me)]
   [[paper](https://arxiv.org/abs/2503.08102)]

5. **[Congee](https://www.cognee.ai/)**
   [[code](https://github.com/topoteretes/cognee)]
   [[paper](https://arxiv.org/abs/2505.24478)]
   [[blog](https://www.cognee.ai/blog)]

6. **[MemOS](https://memos.openmem.net/)** 
   [[code](https://github.com/MemTensor/MemOS)]
   [[paper](https://arxiv.org/abs/2507.03724)]
   [[blog](https://supermemory.ai/blog)]
   
7. **[MemU](https://memu.pro/)**
   [[code](https://github.com/NevaMind-AI/memU)]
   [[blog](https://memu.pro/blog)]

8. **[MIRIX](https://mirix.io/)**
   [[code](https://github.com/Mirix-AI/MIRIX)]
   [[paper](https://arxiv.org/abs/2507.07957)]
   [[blog](https://mirix.io/blog)]

9. **[Memobase](https://memobase.io/)**
   [[code](https://github.com/memodb-io/memobase)]
   [[blog](https://www.memobase.io/blog)]

10. **[LangMem (part of LangChain-LangGraph)](https://langchain-ai.github.io/langmem/)**
    [[code](https://github.com/langchain-ai/langmem)]
    [[blog](https://blog.langchain.com/)]

11. **[MemoryOS](https://baijia.online/memoryos/)**
      [[code](https://github.com/BAI-LAB/MemoryOS)]
      [[paper](https://arxiv.org/pdf/2506.06326)]

12. **[EverMemOS (part of EverMind)](https://evermind-ai.com/)**
    [[code](https://github.com/EverMind-AI/EverMemOS/)]
    [[blog](https://evermind-ai.com/blog/)]

13. **[TeleMem (under construction)]()**
    [[code](https://github.com/TeleAI-UAGI/TeleMem)]

    _To be realeased soon. Stay tuned._

#### Closed-Source

1. **[Supermemory](https://supermemory.ai/)**
   [[partial-code](https://github.com/supermemoryai/supermemory)]
   [[blog](https://supermemory.ai/blog)]

2. **[Memories.ai](https://memories.ai/)**
   [[research](https://memories.ai/research)]
   [[paper](https://memories.ai/research/Camera)]
   [[blog](memories.ai/blogs)]

4. **[Mem 2.0](https://get.mem.ai/)**
   [[blog](https://get.mem.ai/blog)]

5. **[TwinMind](https://twinmind.com/)**

#### Archival (Inactive)

1. [Memvid](https://www.memvid.com/)
   [[code](https://github.com/Olow304/memvid)]
   [[critique1](https://github.com/Olow304/memvid/issues/63),[critique2](https://github.com/Olow304/memvid/issues/49)]

2. [Memary](https://kingjulio8238.github.io/memarydocs/)
   [[code](https://github.com/kingjulio8238/memary)]

<h2 id="tutorials">üßë‚Äçüè´ Tutorials</h2>

TBA

<h2 id="surveys">üìö Surveys</h2>

#### üóìÔ∏è 2025

- **[From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs](https://arxiv.org/abs/2504.15965)**

- **[A Survey on the Memory Mechanism of Large Language Model based Agents](https://dl.acm.org/doi/10.1145/3748302)**
   [[code](https://github.com/nuster1128/LLM_Agent_Memory_Survey)]

- **[Cognitive Memory in Large Language Models](https://arxiv.org/abs/2504.02441)**

- **[Advances and Challenges in Foundation Agents: From Brain-Inspired Intelligence to Evolutionary, Collaborative, and Safe Systems (Chapter 3)](https://arxiv.org/abs/2504.01990)**

- **[Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions](https://arxiv.org/abs/2505.00675)**

- **[Human-inspired Perspectives: A Survey on AI Long-term Memory](https://arxiv.org/abs/2411.00489)**
   
<h2 id="benchmarks">üìè Benchmarks</h2>

### üí¨ Text-only Benchmarks

#### üóìÔ∏è 2025

- **[MOOM: Maintenance, Organization and Optimization of Memory in Ultra-Long Role-Playing Dialogues](https://arxiv.org/abs/2509.11860)**
  [[code](https://github.com/cows21/MOOM-Roleplay-Dialogue)]
  [[data](https://github.com/cows21/MOOM-Roleplay-Dialogue/tree/main/data)]
  
- **[HaluMem: Evaluating Hallucinations in Memory Systems of Agents](http://arxiv.org/abs/2511.03506)**
   [[code](https://github.com/MemTensor/HaluMem)]
   [[data](https://huggingface.co/datasets/IAAR-Shanghai/HaluMem)]

- **[MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems](https://arxiv.org/abs/2510.17281)**
   [[code](https://github.com/LittleDinoC/MemoryBench)]
   [[data](https://huggingface.co/datasets/THUIR/MemoryBench)]

- **[OdysseyBench: Evaluating LLM Agents on Long-Horizon Complex Office Application Workflows](https://arxiv.org/abs/2508.09124)**
   
- **[LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory](https://arxiv.org/abs/2410.10813)**
   [[data](https://github.com/xiaowu0162/LongMemEval)]
   
- **[LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks](https://arxiv.org/abs/2412.15204)**
   [[data](https://github.com/THUDM/LongBench)]

- **[Minerva: A Programmable Memory Test Benchmark for Language Models](https://arxiv.org/abs/2502.03358)**

#### üóìÔ∏è 2024

- **[‚àûBench: Extending Long Context Evaluation Beyond 100K Tokens](https://arxiv.org/abs/2402.13718v3)**
    [[code](https://github.com/OpenBMB/InfiniteBench)]

- **[Evaluating Very Long-Term Conversational Memory of LLM Agents](https://arxiv.org/abs/2402.17753)** (The LoCoMo Paper)
    [[code](https://github.com/snap-research/LoCoMo)]
    [[data](https://github.com/snap-research/locomo/tree/main/data)]

- **[LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding](https://arxiv.org/abs/2308.14508)**
    [[code](https://github.com/THUDM/LongBench/blob/main/LongBench/README.md)]

#### üóìÔ∏è 2023
-  **[Storybench: A multifaceted benchmark for continuous story visualization ](https://proceedings.neurips.cc/paper_files/paper/2023/hash/f63f5fbed1a4ef08c857c5f377b5d33a-Abstract-Datasets_and_Benchmarks.html)**
   [[code](https://github.com/google/storybench)]

### üé¨ Multimodal Benchmarks

#### üóìÔ∏è 2025

-  **[TeleEgo: Benchmarking Egocentric AI Assistants in the Wild](https://arxiv.org/abs/2510.23981)**
    [[code](https://github.com/TeleAI-UAGI/TeleEgo)] [[project](https://programmergg.github.io/jrliu.github.io/)]

-  **[LVBench: An Extreme Long Video Understanding Benchmark](https://arxiv.org/abs/2406.08035)**
    [[code](https://github.com/zai-org/LVBench)]

- **[Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis](https://arxiv.org/abs/2405.21075v3)**
   [[code](https://github.com/MME-Benchmarks/Video-MME)]

- **[Task-Core Memory Management and Consolidation for Long-term Continual Learning](https://arxiv.org/abs/2505.09952)**

#### üóìÔ∏è 2024

-  **[MovieChat+: Question-aware Sparse Memory for Long Video Question Answering](https://arxiv.org/abs/2404.17176)**
    [[code](https://github.com/rese1f/MovieChat)]

-  **[CinePile: A Long Video Question Answering Dataset and Benchmark](https://arxiv.org/abs/2405.08813)**
    [[code](https://huggingface.co/datasets/tomg-group-umd/cinepile)]

-  **[LongVideoBench: A Benchmark for Long-Context Interleaved Video-Language Understanding](https://arxiv.org/abs/2407.15754)**
   [[code](https://github.com/longvideobench/LongVideoBench)]

#### üóìÔ∏è 2023

- **[LvBench: A Benchmark for Long-form Video Understanding with Versatile Multi-modal Question Answering](https://arxiv.org/abs/2312.04817)**

- **[EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding](https://proceedings.neurips.cc/paper_files/paper/2023/file/90ce332aff156b910b002ce4e6880dec-Paper-Datasets_and_Benchmarks.pdf)** [[code](https://github.com/egoschema/egoschema)]

<h2 id="text-memory">üìù Papers - Text Memory</h2>

### üìñ Plain-Text based Memory

#### üóìÔ∏è 2025

- **[LightMem: Lightweight and Efficient Memory-Augmented Generation](https://arxiv.org/abs/2510.18866)**
   [[code](https://github.com/zjunlp/LightMem)]

- **[Nemori: Self-Organizing Agent Memory Inspired by Cognitive Science](https://arxiv.org/abs/2508.03341)**
    [[code](https://github.com/nemori-ai/nemori)]

- **[Omne-R1: Learning to Reason with Memory for Multi-hop Question Answering](https://arxiv.org/abs/2508.17330)**

- **[In Prospect and Retrospect: Reflective Memory Management for Long-term Personalized Dialogue Agents](https://aclanthology.org/2025.acl-long.413/)**

- **[SEDM: Scalable Self-Evolving Distributed Memory for Agents](https://arxiv.org/abs/2509.09498)**

- **[MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced Retrieval Augmentation](https://arxiv.org/abs/2409.05591)**

- **[Human-inspired Episodic Memory for Infinite Context LLMs](https://arxiv.org/abs/2407.09450)**

- **[Towards LifeSpan Cognitive Systems](https://arxiv.org/abs/2409.13265)**

#### üóìÔ∏è 2024

- **[Compress to Impress: Unleashing the Potential of Compressive Memory in Real-World Long-Term Conversations](https://arxiv.org/abs/2402.11975)** [[code](https://github.com/nuochenpku/COMEDY)]

- **[Agent Workflow Memory](https://arxiv.org/abs/2409.07429)** [[code](https://github.com/zorazrw/agent-workflow-memory)]

- **[MemoryBank: Enhancing Large Language Models with Long-Term Memory](https://ojs.aaai.org/index.php/AAAI/article/view/29946)** [[code](https://github.com/zhongwanjun/MemoryBank-SiliconFriend)]

- **[InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory](https://arxiv.org/abs/2402.04617)**

#### üóìÔ∏è 2023

- **[RET-LLM: Towards a General Read-Write Memory for Large Language Models](https://arxiv.org/abs/2305.14322)**

### üß† Graph based Memory

#### üóìÔ∏è 2025

- **[From Experience to Strategy: Empowering LLM Agents with Trainable Graph Memory](https://www.arxiv.org/abs/2511.07800)**

- **[From RAG to Memory: Non-Parametric Continual Learning for Large Language Models](https://arxiv.org/abs/2502.14802)**
    [[code](https://github.com/OSU-NLP-Group/HippoRAG)]

- **[MIRIX: Multi-Agent Memory System for LLM-Based Agents](https://arxiv.org/abs/2507.07957)**
    [[code](https://github.com/Mirix-AI/MIRIX)]

- **[Hierarchical Memory Organization for Wikipedia Generation](https://aclanthology.org/2025.acl-long.1423/)**
    [[code](https://github.com/eugeneyujunhao/mog)]

- **[Bridging Intuitive Associations and Deliberate Recall: Empowering LLM Personal Assistant with Graph-Structured Long-term Memory](https://aclanthology.org/2025.findings-acl.901/)**

- **[HiAgent: Hierarchical Working Memory Management for Solving Long-Horizon Agent Tasks with Large Language Model](https://aclanthology.org/2025.acl-long.1575/)**

- **[Optimizing the Interface Between Knowledge Graphs and LLMs for Complex Reasoning](https://arxiv.org/abs/2505.24478)**

#### üóìÔ∏è 2024

- **[HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models](https://arxiv.org/abs/2405.14831)**
    [[code](https://github.com/OSU-NLP-Group/HippoRAG)]

- **[AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents](https://arxiv.org/abs/2407.04363)**
    [[code](https://github.com/AIRI-Institute/AriGraph)]

<h2 id="parameter-memory">‚öôÔ∏è Papers - Parameter Memory</h2>

#### üóìÔ∏è 2025

- **[R<sup>3</sup>Mem: Bridging Memory Retention and Retrieval via Reversible Compression](https://arxiv.org/abs/2502.15957)**

- **[May the Memory Be With You: Efficient and Infinitely Updatable State for Large Language Models](https://dl.acm.org/doi/abs/10.1145/3721146.3721951)**

- **[MLP Memory: Language Modeling with Retriever-pretrained External Memory](https://arxiv.org/abs/2508.01832)**

- **[Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models](https://www.arxiv.org/abs/2508.09874)**

- **[MeMo: Towards Language Models with Associative Memory Mechanisms](https://aclanthology.org/2025.findings-acl.785/)**

- **[REFRAG: Rethinking RAG based Decoding](https://arxiv.org/abs/2509.01092)**

- **[EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts](https://aclanthology.org/2025.acl-long.574/)**

- **[Disentangling Memory and Reasoning Ability in Large Language Models](https://aclanthology.org/2025.acl-long.84/)**

#### üóìÔ∏è 2024

- **[InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory](https://arxiv.org/abs/2402.04617)**
    [[code](https://github.com/thunlp/InfLLM)]

- **[Memory<sup>3</sup>: Language Modeling with Explicit Memory](https://arxiv.org/abs/2407.01178v1)**

- **[MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding](https://openaccess.thecvf.com/content/CVPR2024/papers/He_MA-LMM_Memory-Augmented_Large_Multimodal_Model_for_Long-Term_Video_Understanding_CVPR_2024_paper.pdf)**
    [[code](https://github.com/boheumd/MA-LMM)]

- **[MemoryLLM: Towards Self-Updatable Large Language Models](https://arxiv.org/abs/2402.04624)**
    [[code](https://github.com/wangyu-ustc/MemoryLLM)]

- **[WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models](https://arxiv.org/abs/2405.14768)**
    [[code](https://github.com/zjunlp/EasyEdit)]

- **[Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache](https://arxiv.org/abs/2401.02669)**

- **[MemServe: Context Caching for Disaggregated LLM Serving with Elastic Memory Pool](https://arxiv.org/abs/2406.17565)**

- **[WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models](https://arxiv.org/abs/2405.14768/)**

#### üóìÔ∏è 2023

- **[Augmenting Language Models with Long-Term Memory](https://arxiv.org/abs/2306.07174)**
    [[code](https://github.com/Victorwz/LongMem)]

- **[Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180)**
    [[code](https://github.com/vllm-project/vllm)]

<h2 id="multimodal-memory">üé• Papers - Multimodal Memory</h2>

#### üóìÔ∏è 2025

- **[Infinite Video Understanding](https://www.arxiv.org/abs/2507.09068)** 

- **[Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory](https://arxiv.org/abs/2508.09736)**
    [[code](https://github.com/bytedance-seed/m3-agent)]

- **[HippoMM: Hippocampal-inspired Multimodal Memory for Long Audiovisual Event Understanding](https://arxiv.org/abs/2504.10739)**
    [[code](https://github.com/linyueqian/HippoMM)]

- **[Episodic Memory Representation for Long-form Video Understanding](https://arxiv.org/abs/2508.09486)**

- **[Multi-RAG: A Multimodal Retrieval-Augmented Generation System for Adaptive Video Understanding](https://arxiv.org/abs/2505.23990)**

- **[Contextual Experience Replay for Self-Improvement of Language Agents](https://arxiv.org/abs/2506.06698)**

#### üóìÔ∏è 2024

- **[VideoAgent: Long-form Video Understanding with Large Language Model as Agent](https://arxiv.org/abs/2403.10517)**
    [[code](https://github.com/HKUDS/VideoAgent)]

- **[VideoChat-Flash: Hierarchical Compression for Long-Context Video Modeling](https://arxiv.org/abs/2501.00574)**
    [[code](https://github.com/OpenGVLab/VideoChat-Flash)]

- **[LongVLM: Efficient Long Video Understanding via Large Language Models](https://arxiv.org/abs/2404.03384)**
    [[code](https://github.com/ziplab/LongVLM)]

<h2 id="memory-retrieval">üîç Papers - Memory Retrieval</h2>

### üß≠ Reinforcement Learning

#### üóìÔ∏è 2025

- **[FLEX: Continuous Agent Evolution via Forward Learning from Experience](https://arxiv.org/abs/2511.06449)**
   [[code](https://github.com/GenSI-THUAIR/FLEX)]

-  **[ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory](https://arxiv.org/abs/2509.25140)**

-  **[Memento: Fine-tuning LLM Agents without Fine-tuning LLMs](https://arxiv.org/abs/2508.16153)**
   [[code](https://github.com/Agent-on-the-Fly/Memento)]
   
-  **[REFRAG: Rethinking RAG based Decoding](https://arxiv.org/abs/2509.01092)**

-  **[Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory](https://arxiv.org/abs/2508.09736)**
     [[code](https://github.com/bytedance-seed/m3-agent)]

-  **[MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent](https://arxiv.org/abs/2507.02259)**

-  **[MemGen: Weaving Generative Latent Memory for Self-Evolving Agents](https://arxiv.org/abs/2509.24704)**

-  **[ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization](https://arxiv.org/abs/2509.13313)**

- **[MARC: Memory-Augmented RL Token Compression for Efficient Video Understanding](https://arxiv.org/pdf/2510.07915)**

<h2 id="articles">üì∞ Articles</h2>

#### üóìÔ∏è 2025

- **[Survey of AI Agent Memory Frameworks](https://www.graphlit.com/blog/survey-of-ai-agent-memory-frameworks)**

#### üóìÔ∏è 2024

- **[Memory in Language Model-Enabled Agents](https://yuweisunn.github.io/blog-1-06-24.html)**

- **[Mastering LLM Memory: A Comprehensive Guide](https://www.strongly.ai/blog/mastering-llm-memory-a-comprehensive-guide.html)**

#### üóìÔ∏è 2023

- **[LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/)**

<h2 id="workshops">üí¨ Workshops</h2>

#### üóìÔ∏è 2025

- **[Proceedings of the First Workshop on Large Language Model Memorization (L2M2)](https://aclanthology.org/volumes/2025.l2m2-1/)**

